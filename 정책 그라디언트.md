#  OpenAI 짐 설치

### <U> Note. 본 글은 (1) 오렐리아 제롱의 "핸즈온 머신러닝 (2판)", (2) 이웅렬 외 4인의 "파이썬과 케라스로 배우는 강화학습", (3) 리처트 서튼 & 앤드류 바르토의 "단단한 강화학습"을 참고하여 작성되었습니다. </U>

## 1) OpenAI 짐 설치

강화학습에서 어려운 점은 에이전트를 훈련시키는 작업 환경을 마련해야 한다는 것이다. 그리고 아마 이것이 현재 많은 강화학습 연구들이 현실의 문제해결에 적용되는 방향이 아니라 Benchmark 문제를 해결하는 방향으로 진행되는 이유일 것이다. 

필자는 "강화학습 연구"가 (현실문제와 많이 동떨어진) 현실의 문제가 단순화 된 Benchmark 문제를 대상으로 추상적 이론연구에 전착하는 것을 경계해야 하며 복잡한 현실의 문제를 해결했을 때 비로소 의미가 있다고 보는 입장이다. 

한편 "강화학습 공부"의 관점에서 보면 얘기가 달라진다. 고전 이론부터 현대의 고도화 된 이론까지 전부 이해하기도 벅찬 마당에 복잡한 현실의 문제까지 고려하는 것은 입문자에게 끔찍한 일이라고 생각한다. 즉, 공부를 위해선 Benchmark 문제를 활용하는 것이 오히려 유효하다고 생각한다. 

따라서 앞으로 강화학습을 공부하며 작성한 내용과 코드들은 Benchmark 문제에 기반할 것이다.  강화학습 시뮬레이션 환경을 제공하는 오픈소스 라이브러리 OpenAI gym은 강화학습이 적용되는 대표적인 Benchmark 문제 환경들 (예. 아타리 게임, 보드 게임, 2D와 3D 물리 시뮬레이션 등)을 제공한다. 

아나콘다 서버 유저의 경우 터미널에 `conda activate 'name_virtualenv'`를 입력하여 OpenAI gym이 설치될 가상환경을 활성화 해준뒤 `pip install -user gym`를 입력하면 파이썬에 OpenAI gym 환경을 설치할 수 있다. 그 다음 아래와 같은 코드를 실행하면 강화학습 오픈소스 시뮬레이션 환경을 불러올 수 있다. 
```python
import gym
env = gym.make("CartPole-v1") # gym.make("환경 명") : 환경을 선언해준다. 
obs = env.reset() # 환경을 초기화해준다.
obs
env.action_space # 호출된 환경 (현재는 "CartPole-v1")에서 가능한 행위공간을 보여준다.
                 # Discrete(2)는 가능한 행동이 0, 1 두 가지 정수임을 의미한다.
```
- 필자는 `CartPole-v1` 환경을 불러보았다. 
- `env.reset()`을 통해 환경내 모든 변수를 초기화 해주었다.
- `obs`는 환경에서 관측되는 "상태"를 의미한다. `obs`를 실행하면 4 개의 실수를 담은 1D 넘파이 배열 `array([x, y, z, w])`을 반환하는데 x, y, z, w는 각각은 (1) 카트의 수평 위치, (2) 카트의 속도, 막대의 각도, 막대의 각속도를 나타낸다. 
- `env.action_space`를 통해 주어진 문제에서 정의된 행위공간을 확인할 수 있다.


아래의 코드는 정책함수 (Policy)를 정의한 뒤 정책함수를 따라 결정되는 매 step의 행위 (aciton)를 결정하여 500회의 에피소드를 수행하는 코드이다.

```python
# %%
# ! main() 메인함수가 정의된 script block은 반드시 python script를 실행하여 구동하여야 한다.
# ! 그 이유는 Jupyter Notebook, vs-code 등 프로그래밍 익스텐션 내에는 
# ! 그래픽 라이브러리 

# 정책함수 정의
def basic_policy(obs):
    angle = obs[2] # obs[2]는 카트 위 막대의 각도 (angle)에 대한 수치이다. 
                   # 막대가 수직일 시 obs[2] = 0
                   # 왼쪽으로 기울어질 시 obs[2] = 음수
                   # 오른쪽으로 기울어질 시 obs[2] = 양수

                   # 본 정책함수는 보상 (reward)인자가 활용되지 않으므로 
                   # 보상을 '학습'이 강화되지 않는 정책이라고 할 수 있음.

    return 0 if angle < 0 else 1 # obs[2]가 음수이면 0을 반환, 양수이면 1을 반환한다.

# 메인 함수 (python script 실행시 호출되는 함수)
def main():
    env = gym.make("CartPole-v1") # gym.make("환경 명") : 환경을 선언해준다. 
    totals = [] # 각 에피소드 벌 평균 보상을 담을 변수 totals 정의
       
    for episode in range(500): # 에피소드 500회 실행
        episode_rewards = 0 # 에피소드 보상 초기화
        obs = env.reset() # 환경 초기화
                          # env.reset()이 반환하는 obs값은 크기가 4인 1D 배열이다.
                          # 배열 내 4개의 변수는 각각 카트위치, 카트속도, 막대 (폴)각도, 막대 (폴)각속도를 의미한다.
                          ## "속도"는 속력 x 방향으로 카트속도, 막대각속도는 속력과 방향 정보를 모두 내포한다.

        print('episode : ', episode) # 몇 회째 에피소드 진행중인가 출력

        while True: # 각 에피소드 별로 env.step 무한번 실행
            action = basic_policy(obs) # 정책함수로부터 이전 obs[2]가 양수냐 음수냐에 따라 
                                       # 0 혹은 1을 반환하여 다음 행위(action)를 정의

            obs, reward, done, info = env.step(action) # env.step() 함수는 action을 인자로 받는다.
                                                       # env.step()은 인자로 받은 action을 수행했을 때 
                                                       # 다음 상태 (obs), 보상 (reward), 에피소드 엔딩여부 (done) 등을 반환한다.

            env.render() # env.render()는 새로운 상태가 진행된 현재 환경을 렌더링한다.

            episode_rewards += reward # 각 에피소드 별 보상 (reward)의 총합을 계산한다.

            if done: # 에피소드 종료 여부가 True라면 현 에피소드를 종료하고 다음 에피소드로 넘어간다.
                break

        totals.append(episode_rewards) # 각 에피소드별 보상 총합을 totals라는 리스트에 담는다.
        print(np.mean(totals), np.std(totals), np.min(totals), np.max(totals)) # 모든 에피소드의 보상 총합의 누적 평균을 계산한다.

        env.close() # 환경을 종료하고 창을 닫는다.

# 실행구조 작성
if __name__ == "__main__": # __name__ 은 python에서 내부적으로 사용하는 특별한 변수이다. python script.py 파일을 실행할 때 __name__ 변수에 " __main__" 값이 자동으로 할당된다.
                           # 반면 import script.py를 실행할 때는 __name__ 변수에 "script"가 할당된다. 즉 script.py을 실행하지 않고 안에 작성된 모듈들만 활성화 (import) 할 수 있다.
                           # 요약하자면, 모듈 활성화와 실행을 구분하기 위해 if __name__ = "__main__" 구문을 마지막에 추가한다고 생각하면 된다.

    main()  # 본 script 파일을 실행하면 (python Policy_Gradient.py) main() 함수를 호출하여라.
```

위의 코드를 실행하면 [그림1]처럼 카트폴이 양옆으로 움직이며 중심을 잡는 시뮬레이션 과정이 실행된다.

<p align = "center"><img src = "https://user-images.githubusercontent.com/61273017/83596259-2bb6ba00-a59f-11ea-821c-c439f2d458aa.png" width = "600" height = "200"></p>
<p align = "center">[그림1] 카트폴 시뮬레이션 </p>

## 2) 신경망 정책

이제 본격적으로 뉴럴 네트워크, 즉 신경망을 활용하여 정책을 학습해보자. 신경망을 활용하여 학습된 정책을 **신경망 정책** 이라고 한다. 신경망은 이전 상태 (크기 4의 1D 배열)를 입력으로 받아 가능한 행동들에 대한 확률벡터를 출력한다. 신경망 내부에서 추정되는 **각 레이어의 파라미터 (weight)들**이 **정책 파라미터**가 되며 이 파라미터들을 최적화하는 과정을 통해 정책을 학습한다. 

앞서 `env.action_space`를 통해 확인한 바와 같이 카트폴 환경에서 가능한 행위는 {0 : 왼쪽이동, 1 : 오른쪽이동}으로 총 2가지이다. 다시 말해 어떤 하나의 행위를 선택할 확률만 구하면 (확률이니까) 나머지 행동을 선택할 확률은 자연히 구해진다. 이를 위해 출력층 바로 전에 `sigmoid` 활성화층을 적용하여 확률을 구하면 된다. 그렇다면 왜 이런 확률적 접근법을 취하는 것일까?

이와 같은 확률적 접근을 취하는 데에는 이유가 있다. 만일 예측 된 점수 (확률 혹은 Q-value) 중 가장 높은 값을 갖는 행동을 선택하도록 강제 (의사결정과정이 greedy search를 따르도로 강제)한다면 신경망에 학습되는 정책 파라미터 (weight)들은 랜덤성이 반영된 확률과정의 결과물이 아니라 가장 높은 값을 탐욕적으로 추구하는 greedy policy에 의한 결과물이 된다. 즉 greedy policy라는 행동 정책을 결정해놓고 이 행동 정책을 따르는 정책 파라미터를 학습하게 되는 것이다. 이렇게 랜덤성을 배제하면 새로운 행동을 "탐험 (exploration)" 하지 않고 잘 할수 있는 행동을 "활용 (exploitation)"하는 것에만 집중하게 된다. 이 경우 local optimum에 빠질 가능성이 아주 높아지므로, local optimum에서 탈출할 수 있도록 "탐험"을 장려하는 확률적 접근법을 취하는 것이다.

[그림 2]는 카트폴 시뮬레이션 환경에서 정책 그라디언트의 학습 과정을 나타낸다.

<p align = "center"><img src = "https://user-images.githubusercontent.com/61273017/83599202-8c95c080-a5a6-11ea-8e71-1ad87058781b.png" width = 400 height = 500></p> <p align = "center"> [그림 2] 정책 그라디언트 기반 학습 과정과 카트폴 시뮬레이션 환경 </p>
